{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "nwovjY3hnng6",
        "0EGv_iM8Hr4Z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requirements\n",
        "\n"
      ],
      "metadata": {
        "id": "UigRyg1O7Qdz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsgbH5G569E8",
        "outputId": "ebb66cc0-8b5e-4c76-89cc-a287909c2bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=c78e9768bd23a3bf61951f894be007120bd23d9eea6f7b8d4abfe064c38bf066\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "c-bWB3gn7OWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.window import Window\n"
      ],
      "metadata": {
        "id": "Xm7ZPRND7A7i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "AkkIFKhh7UAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
        "test_file = os.path.join(root_dir,'content', 'coronavirus_2023.csv')\n",
        "test_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8alYiO977HDX",
        "outputId": "2b80e0eb-e609-4087-ee55-3e113d7a98b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/coronavirus_2023.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Data Quality Check\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "pjVdFbX1_o5E"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read DF To perform Data Quality Check\n",
        "df = spark.read.csv(test_file, header=True, inferSchema=True)\n",
        "df.show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gamFkjkGAPf5",
        "outputId": "b459a176-445a-4780-e282-a0e79c6e77d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+\n",
            "|      date|province|country|    lat|     long|     type|cases|  uid|iso2|iso3|code3|   combined_key|population|continent_name|continent_code|\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+\n",
            "|2023-01-01| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check date format"
      ],
      "metadata": {
        "id": "FRQ5_7VWJNgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if date have correct format\n",
        "def check_date_format(df, column_to_check, date_format):\n",
        "\n",
        "  # Convert date column to string\n",
        "  formatted_date_column = f.date_format(df[column_to_check], date_format).cast(StringType())\n",
        "\n",
        "  # Conditional check if format of column is the same as the one specified in date_format\n",
        "  is_same_format_condition = f.when(formatted_date_column != df[column_to_check], df[column_to_check])\n",
        "\n",
        "  # Filter df with correct/incorrect formats\n",
        "  correct_format_df = df.filter(is_same_format_condition.isNull())\n",
        "  incorrect_format_df = df.filter(is_same_format_condition.isNotNull())\n",
        "\n",
        "    # Change return accordingly\n",
        "  return correct_format_df, incorrect_format_df"
      ],
      "metadata": {
        "id": "PSmfe6DWGDbm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_format_df, incorrect_format_df = check_date_format(df, 'date', 'yyyy-MM-dd')\n",
        "correct_format_df.show()\n",
        "incorrect_format_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYIoWkyYC7OW",
        "outputId": "4c0bd920-08c3-45d9-84b5-046297f8f9af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+\n",
            "|      date|province|country|    lat|     long|     type|cases|  uid|iso2|iso3|code3|   combined_key|population|continent_name|continent_code|\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+\n",
            "|2023-01-01| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-02| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-03| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-04| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-05| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-06| Alberta| Canada|53.9333|-116.5765|confirmed| 2046|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-07| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-08| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-09| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-10| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-11| Alberta| Canada|53.9333|-116.5765|confirmed|  646|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-12| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-13| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-14| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-15| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-16| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-17| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-18| Alberta| Canada|53.9333|-116.5765|confirmed|  828|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-19| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "|2023-01-20| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----+--------+-------+---+----+----+-----+---+----+----+-----+------------+----------+--------------+--------------+\n",
            "|date|province|country|lat|long|type|cases|uid|iso2|iso3|code3|combined_key|population|continent_name|continent_code|\n",
            "+----+--------+-------+---+----+----+-----+---+----+----+-----+------------+----------+--------------+--------------+\n",
            "+----+--------+-------+---+----+----+-----+---+----+----+-----+------------+----------+--------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check No duplicates"
      ],
      "metadata": {
        "id": "fIqe0RNAeM0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a column doesn't have duplicates\n",
        "def check_no_duplicates(df, column_to_check):\n",
        "\n",
        "    # Create a Window partition for all the unique values of the column to check\n",
        "    window_partition = Window().partitionBy(column_to_check)\n",
        "\n",
        "    # Add column with frequency per each unique value\n",
        "    df_with_frequency = df.withColumn('Frequency', f.count(column_to_check).over(window_partition))\n",
        "\n",
        "    # Filter rows where the count is greater than 1 (indicating duplicates)\n",
        "    df_wth_duplicates = df_with_frequency.filter(f.col('Frequency') > 1)\n",
        "    df_no_duplicates = df_with_frequency.filter(f.col('Frequency') == 1)\n",
        "    # Change return accordingly\n",
        "    return df_wth_duplicates, df_no_duplicates\n"
      ],
      "metadata": {
        "id": "38xDbrFeH4nW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_wth_duplicates, df_no_duplicates = check_no_duplicates(df, 'province')\n",
        "df_wth_duplicates.show()\n",
        "df_no_duplicates.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoWDjh4yc1Uz",
        "outputId": "4618b1c4-77ee-4dc7-ba29-cdeb0981aa8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+---------+\n",
            "|      date|province|country|    lat|     long|     type|cases|  uid|iso2|iso3|code3|   combined_key|population|continent_name|continent_code|Frequency|\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+---------+\n",
            "|2023-01-01| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-02| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-03| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-04| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-05| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-06| Alberta| Canada|53.9333|-116.5765|confirmed| 2046|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-07| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-08| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-09| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-10| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-11| Alberta| Canada|53.9333|-116.5765|confirmed|  646|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-12| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-13| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-14| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-15| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-16| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-17| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-18| Alberta| Canada|53.9333|-116.5765|confirmed|  828|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-19| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "|2023-01-20| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146| North America|            NA|      136|\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+---------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----+--------+-------+---+----+----+-----+---+----+----+-----+------------+----------+--------------+--------------+---------+\n",
            "|date|province|country|lat|long|type|cases|uid|iso2|iso3|code3|combined_key|population|continent_name|continent_code|Frequency|\n",
            "+----+--------+-------+---+----+----+-----+---+----+----+-----+------------+----------+--------------+--------------+---------+\n",
            "+----+--------+-------+---+----+----+-----+---+----+----+-----+------------+----------+--------------+--------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if Null values"
      ],
      "metadata": {
        "id": "vTC4OuPueZyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_null_values(df, column_to_check):\n",
        "    # Filter df with correct/incorrect formats\n",
        "  df_with_null = df.filter(df[column_to_check].isNull())\n",
        "  df_without_null = df.filter(df[column_to_check].isNotNull())\n",
        "  return df_with_null, df_without_null\n"
      ],
      "metadata": {
        "id": "QeRDZVlGeZlA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_wth_duplicates = df_wth_duplicates.withColumn('continent_name', f.when(f.col('continent_name') == 'North America', None).otherwise(f.col('continent_name')))\n",
        "df_wth_duplicates.show(1)\n",
        "df_with_null, df_without_null = check_null_values(df_wth_duplicates, 'continent_name')\n",
        "df_with_null.show(1)\n",
        "df_without_null.show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP4INtQEfD5l",
        "outputId": "d7ad57d3-3b08-400a-eef9-a90a9c163d5f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+---------+\n",
            "|      date|province|country|    lat|     long|     type|cases|  uid|iso2|iso3|code3|   combined_key|population|continent_name|continent_code|Frequency|\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+---------+\n",
            "|2023-01-01| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146|          NULL|            NA|      136|\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+---------+\n",
            "only showing top 1 row\n",
            "\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+---------+\n",
            "|      date|province|country|    lat|     long|     type|cases|  uid|iso2|iso3|code3|   combined_key|population|continent_name|continent_code|Frequency|\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+---------+\n",
            "|2023-01-01| Alberta| Canada|53.9333|-116.5765|confirmed|    0|12401|  CA| CAN|  124|Alberta, Canada|   4413146|          NULL|            NA|      136|\n",
            "+----------+--------+-------+-------+---------+---------+-----+-----+----+----+-----+---------------+----------+--------------+--------------+---------+\n",
            "only showing top 1 row\n",
            "\n",
            "+----------+--------+-------+-------+--------+---------+-----+-----+----+----+-----+------------+----------+--------------+--------------+---------+\n",
            "|      date|province|country|    lat|    long|     type|cases|  uid|iso2|iso3|code3|combined_key|population|continent_name|continent_code|Frequency|\n",
            "+----------+--------+-------+-------+--------+---------+-----+-----+----+----+-----+------------+----------+--------------+--------------+---------+\n",
            "|2023-01-01|   Anhui|  China|31.8257|117.2264|confirmed|    2|15601|  CN| CHN|  156|Anhui, China|  63240000|          Asia|            AS|      204|\n",
            "+----------+--------+-------+-------+--------+---------+-----+-----+----+----+-----+------------+----------+--------------+--------------+---------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if columns have duplicates"
      ],
      "metadata": {
        "id": "l_mjZPO3bZ5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, col\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "def check_no_duplicates(df, column):\n",
        "\n",
        "    # Create a Window partition for all the unique values of the columns\n",
        "    window_partition = Window().partitionBy(column)\n",
        "\n",
        "    # Add Frequency column to check if there are duplicated rows\n",
        "    df_with_frequency = df.withColumn('Frequency', count('*').over(window_partition))\n",
        "\n",
        "    # Check if df have duplicated values in the column\n",
        "    num_of_rows = df_with_frequency.filter(col('Frequency') > 1).count()\n",
        "    if num_of_rows == 0:\n",
        "      print(f'no duplicated values in column: {column}')\n",
        "    if num_of_rows > 0:\n",
        "      print(f' {num_of_rows} duplicated rows in column: {column}. Check below the duplicated row')\n",
        "      df_with_frequency.filter(col('Frequency') > 1).drop('Frequency').show(1)\n",
        "      # return df_with_frequency.filter(col('Frequency') > 1).drop('Frequency')\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "dKz4fLyubfKF"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CheckNoDuplicatesTest\").getOrCreate()\n",
        "\n",
        "# Create example DataFrame\n",
        "\n",
        "data = [(1, \"John\", 30), (2, \"Alice\", 25), (3, \"Bob\", 35), (4, \"Dave\", 30), (4, \"Dave\", 30)]\n",
        "columns = [\"id\", \"name\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Specify columns to check for duplicates\n",
        "specified_columns = 'id'\n",
        "\n",
        "# Check for duplicates in specified columns\n",
        "check_no_duplicates(df, specified_columns)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHTtBlEPfYEA",
        "outputId": "fe95c5d4-13ab-49e2-d51c-f9d670c3e803"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 2 duplicated rows in column: id. Check below the duplicated row\n",
            "+---+----+---+\n",
            "| id|name|age|\n",
            "+---+----+---+\n",
            "|  4|Dave| 30|\n",
            "+---+----+---+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if column values are within range\n"
      ],
      "metadata": {
        "id": "-OS40E4JbfYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_column_range(df, column, min_value, max_value):\n",
        "    # Check if df have values outside the range\n",
        "    num_of_rows = df.filter((col(column) < min_value) | (col(column) > max_value)).count()\n",
        "    if num_of_rows == 0:\n",
        "      print(f'dataframe does not have values outside the range: {min_value}-{max_value} for the column: {column}')\n",
        "    else:\n",
        "      print(f'dataframe does have values outside the range: {min_value}-{max_value} for the column: {column}. Check rows below')\n",
        "      df.filter((col(column) < min_value) | (col(column) > max_value)).show()\n",
        "\n"
      ],
      "metadata": {
        "id": "XPDoN7snbsAe"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"CheckColumnRangeTest\").getOrCreate()\n",
        "\n",
        "# Create example DataFrame\n",
        "data = [(1, \"John\", 30), (2, \"Alice\", 25), (3, \"Bob\", 35), (4, \"Dave\", 40)]\n",
        "columns = [\"id\", \"name\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Specify the column and range to check\n",
        "column_to_check = \"age\"\n",
        "min_age = 20\n",
        "max_age = 35\n",
        "\n",
        "# Run the check\n",
        "check_column_range(df, column_to_check, min_age, max_age)\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--1DwvdRtz8j",
        "outputId": "1619306b-6cd6-442e-953f-114edfd31f54"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataframe does have values outside the range: 20-35 for the column: age. Check rows below\n",
            "+---+----+---+\n",
            "| id|name|age|\n",
            "+---+----+---+\n",
            "|  4|Dave| 40|\n",
            "+---+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if string column has lenght within range"
      ],
      "metadata": {
        "id": "Pqjgyhz1bsU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import length\n",
        "\n",
        "def check_string_length_range(df, column, min_length, max_length):\n",
        "\n",
        "  # Check if any row with string lenght\n",
        "  num_of_rows = df.filter((length(col(column)) < min_length) | (length(col(column)) > max_length)).count()\n",
        "  if num_of_rows == 0:\n",
        "      print(f'dataframe does not have values outside the range: {min_length}-{max_length} for the column: {column}')\n",
        "  else:\n",
        "    print(f'dataframe does have values outside the range: {min_length}-{max_length} for the column: {column}. Check rows below')\n",
        "    df.filter((length(col(column)) < min_length) | (length(col(column)) > max_length)).show()\n"
      ],
      "metadata": {
        "id": "E8WOZF1pbxDq"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"CheckStringLengthRangeTest\").getOrCreate()\n",
        "\n",
        "# Create example DataFrame\n",
        "data = [(1, \"John\", 30), (2, \"Aliceee\", 25), (3, \"Bob\", 35), (4, \"Dave\", 40)]\n",
        "columns = [\"id\", \"name\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Specify the string column and length range to check\n",
        "column_to_check = \"name\"\n",
        "min_length = 3\n",
        "max_length = 5\n",
        "\n",
        "# Run the check\n",
        "check_string_length_range(df, column_to_check, min_length, max_length)\n",
        "\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ow0CNq0Vw9jA",
        "outputId": "4a883ec9-6d99-49ba-adf8-63616e7c5b8b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataframe does have values outside the range: 3-5 for the column: name. Check rows below\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  2|Aliceee| 25|\n",
            "+---+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if text matches regex pattern"
      ],
      "metadata": {
        "id": "GXm8kE-XbxSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "def check_column_match_regex(df, column, regex_pattern):\n",
        "  # Check if any row with string lenght\n",
        "  num_of_rows = df.filter(regexp_extract(col(column), regex_pattern, 0) != \"\").count()\n",
        "  if num_of_rows == 0:\n",
        "      print(f'dataframe does not have values with the pattern: {regex_pattern} for the column: {column}')\n",
        "  else:\n",
        "    print(f'dataframe do have values with the pattern: {regex_pattern} for the column: {column}. Check rows below')\n",
        "    df.filter(regexp_extract(col(column), regex_pattern, 0) != \"\").show()\n"
      ],
      "metadata": {
        "id": "zuu2IIwcb6kG"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"CheckColumnRegexTest\").getOrCreate()\n",
        "\n",
        "# Create example DataFrame\n",
        "data = [(1, \"John123\", 30), (2, \"Alice456\", 25), (3, \"Bob\", 35), (4, \"Dave\", 40)]\n",
        "columns = [\"id\", \"name\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Specify the column and regex pattern to check\n",
        "column_to_check = \"name\"\n",
        "regex_pattern = r'\\d+'  # Matches one or more digits\n",
        "\n",
        "# Run the check\n",
        "check_column_match_regex(df, column_to_check, regex_pattern)\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBTWKKdz0D48",
        "outputId": "3daf1a47-2995-42cd-9f29-7e90b44d416a"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataframe do have values with the pattern: \\d+ for the column: name. Check rows below\n",
            "+---+--------+---+\n",
            "| id|    name|age|\n",
            "+---+--------+---+\n",
            "|  1| John123| 30|\n",
            "|  2|Alice456| 25|\n",
            "+---+--------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if string column values are within a list of values"
      ],
      "metadata": {
        "id": "v9po-L1Ub8wC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_column_values_in_list(df, column, value_list):\n",
        "  # Check if any row with string lenght\n",
        "  num_of_rows = df.filter(~col(column).isin(value_list)).count()\n",
        "  if num_of_rows == 0:\n",
        "      print(f'dataframe does not have values within the list of values: {value_list} for the column: {column}')\n",
        "  else:\n",
        "    print(f'dataframe do have values within the list of values: {value_list} for the column: {column}. Check rows below')\n",
        "    df.filter(~col(column).isin(value_list)).show()\n"
      ],
      "metadata": {
        "id": "RLbe6bGNcFUz"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"CheckColumnValuesInListTest\").getOrCreate()\n",
        "\n",
        "# Create example DataFrame\n",
        "data = [(1, \"John\", 30), (2, \"Alice\", 25), (3, \"Bob\", 35), (4, \"Dave\", 40)]\n",
        "columns = [\"id\", \"name\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Specify the string column and list of values to check\n",
        "column_to_check = \"name\"\n",
        "values_to_check = [\"John\", \"Alice\", \"Charlie\"]\n",
        "\n",
        "# Run the check\n",
        "check_column_values_in_list(df, column_to_check, values_to_check)\n",
        "\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqzggURnDlSw",
        "outputId": "5bb552e7-bbb9-4283-b51b-91fc4fc2179d"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataframe do have values within the list of values: ['John', 'Alice', 'Charlie'] for the column: name. Check rows below\n",
            "+---+----+---+\n",
            "| id|name|age|\n",
            "+---+----+---+\n",
            "|  3| Bob| 35|\n",
            "|  4|Dave| 40|\n",
            "+---+----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check if numeric column values are within a list of values (intended for categorical values)"
      ],
      "metadata": {
        "id": "M76dfE_OcHoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Same function as above"
      ],
      "metadata": {
        "id": "RHSJV5xycOvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Number of elements per category"
      ],
      "metadata": {
        "id": "nwovjY3hnng6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"\").count().show()"
      ],
      "metadata": {
        "id": "PqzByV6KnnT6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "f9acb2d0-ac09-4453-9337-8fa8349c9243"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1def2b0bb49e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"GroupedData\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `` cannot be resolved. Did you mean one of the following? [`lat`, `uid`, `date`, `iso2`, `iso3`].;\n'Aggregate ['], [', count(1) AS count#856L]\n+- Relation [date#141,province#142,country#143,lat#144,long#145,type#146,cases#147,uid#148,iso2#149,iso3#150,code3#151,combined_key#152,population#153,continent_name#154,continent_code#155] csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropDuplicates(subset = ['continent_name']).show()"
      ],
      "metadata": {
        "id": "k2TJdPFEdh_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.withColumn(\"lat\",f.col('lat').cast('int'))\n",
        "quantiles = df2.approxQuantile(\"lat\", [0.01, 0.99], 0.0)\n",
        "lower_bound = quantiles[0]\n",
        "upper_bound = quantiles[1]\n",
        "\n",
        "outliers = df2.filter((df2[\"lat\"] < lower_bound) | (df2[\"lat\"] > upper_bound))\n",
        "outliers.show(100)"
      ],
      "metadata": {
        "id": "TZIuk2m1dv6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compare Schemas"
      ],
      "metadata": {
        "id": "0EGv_iM8Hr4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "def compare_schemas(spark, source_df, target_df):\n",
        "    \"\"\"\n",
        "    Compare the schemas of source and target DataFrames.\n",
        "\n",
        "    Parameters:\n",
        "    - spark: PySpark SparkSession\n",
        "    - source_df: PySpark DataFrame representing the source table\n",
        "    - target_df: PySpark DataFrame representing the target table\n",
        "\n",
        "    Returns:\n",
        "    - True if the schemas match, False otherwise\n",
        "    \"\"\"\n",
        "    source_schema = source_df.schema\n",
        "    target_schema = target_df.schema\n",
        "\n",
        "    if source_schema == target_schema:\n",
        "        print(\"Schemas match. Source and target tables have the same structure.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Schemas do not match. Source and target tables have different structures.\")\n",
        "\n",
        "        source_columns = {field.name: field.dataType for field in source_schema.fields}\n",
        "        target_columns = {field.name: field.dataType for field in target_schema.fields}\n",
        "\n",
        "        different_columns = set(source_columns.keys()) ^ set(target_columns.keys())\n",
        "\n",
        "        for column in different_columns:\n",
        "            source_data_type = source_columns.get(column, \"Not present in source\")\n",
        "            target_data_type = target_columns.get(column, \"Not present in target\")\n",
        "\n",
        "            print(f\"Column: {column}, Source Data Type: {source_data_type}, Target Data Type: {target_data_type}\")\n",
        "\n",
        "        return False\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"SchemaComparisonTest\").getOrCreate()\n",
        "\n",
        "# Define schemas for source and target DataFrames\n",
        "source_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "target_schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"score\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Sample data for source and target DataFrames\n",
        "source_data = [(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 22)]\n",
        "target_data = [(1, \"Alice\", 95), (2, \"Bob\", 80), (3, \"Charlie\", 75)]\n",
        "\n",
        "# Create source and target DataFrames\n",
        "source_df = spark.createDataFrame(source_data, schema=source_schema)\n",
        "target_df = spark.createDataFrame(target_data, schema=target_schema)\n",
        "\n",
        "# Print the content of source and target DataFrames\n",
        "print(\"Source DataFrame:\")\n",
        "source_df.show()\n",
        "\n",
        "print(\"Target DataFrame:\")\n",
        "target_df.show()\n",
        "# Compare Schemas\n",
        "compare_schemas(spark, source_df, target_df)\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "_-kGvfrGHwzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create framework for checker"
      ],
      "metadata": {
        "id": "gVO-N9jWHj4W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SbCB2Jo9lD9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3 types of validations. :\n",
        "\n",
        "- Entity\n",
        "- Field\n",
        "- Data\n"
      ],
      "metadata": {
        "id": "2krfKbIaFIUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Check that source column have the same range values as target colum\n",
        "- Check that source column have the same len than target column\n",
        "- Check Column names are same as desired (config file)\n",
        "- verify all field required from source talbe are in target table\n",
        "- Capture constraints defined in sourced table (what constraints do we have? E.g. maybe another config file with a function to find if a number is negative)\n",
        "-"
      ],
      "metadata": {
        "id": "2hJZmkumEH5F"
      }
    }
  ]
}