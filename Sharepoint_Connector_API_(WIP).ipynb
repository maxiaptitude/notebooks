{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozN-RmSO1a4Y",
        "outputId": "05d60710-6af5-4472-a641-1b95c2aaff8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=7799c9f0161405b4e6b90f034fb786021de582acd557f60bf6c43108b28eff08\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pyaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SWCvAi11fxJ",
        "outputId": "5ff27bcf-e4ec-4e62-afdd-0b953df099d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyaml\n",
            "  Downloading pyaml-23.12.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml) (6.0.1)\n",
            "Installing collected packages: pyaml\n",
            "Successfully installed pyaml-23.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install msal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQnjWlP_1ilC",
        "outputId": "490c73c6-2a6b-4ea1-f63d-396cc296d89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting msal\n",
            "  Downloading msal-1.26.0-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from msal) (2.31.0)\n",
            "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /usr/lib/python3/dist-packages (from msal) (2.3.0)\n",
            "Requirement already satisfied: cryptography<44,>=0.6 in /usr/local/lib/python3.10/dist-packages (from msal) (41.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<44,>=0.6->msal) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->msal) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->msal) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->msal) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->msal) (2023.11.17)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<44,>=0.6->msal) (2.21)\n",
            "Installing collected packages: msal\n",
            "Successfully installed msal-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# databricks_utils.py"
      ],
      "metadata": {
        "id": "DVEh59R41nZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to support connector with databricks\n",
        "\n",
        "# Get secrets from Databricks\n",
        "\n",
        "# Extract config variables from Databricks\n",
        "def get_secret_variables():\n",
        "    try:\n",
        "        # Get secret variables from Databricks\n",
        "        client_secret = dbutils.secrets.get(scope = 'scope',key = 'client_secret')\n",
        "        client_id = dbutils.secrets.get(scope = 'scope',key = 'client_id')\n",
        "        authority = dbutils.secrets.get(scope = 'scope',key = 'authority')\n",
        "        scope = dbutils.secrets.get(scope = 'scope',key = 'scope')\n",
        "        return client_secret, client_id,  authority, scope\n",
        "    except Exception as e:\n",
        "        print('''\n",
        "        Databricks secrets were not fetched correctly. please make sure the the variables names are:\n",
        "        - Client Secret = \"client_secret\"\n",
        "        - Client ID = \"client_id\"\n",
        "        - Authority = \"authority\"\n",
        "        - Scope = \"scope\"\n",
        "\n",
        "        For more information, check traceback below:\n",
        "        ''')\n",
        "        print(e)\n",
        "\n"
      ],
      "metadata": {
        "id": "yKPWMWDA1kT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# general_utils.py"
      ],
      "metadata": {
        "id": "Lz1VPpWb1sru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten JSON\n",
        "def flatten_json(json_input, prefix=''):\n",
        "\n",
        "    # Create empty dictionary to collect JSON data\n",
        "    flattened_json_dictionary = {}\n",
        "\n",
        "    # Get key:value items from JSON\n",
        "    for key, value in json_input.items():\n",
        "        # Recursively flatten nested dictionaries within JSON\n",
        "        if isinstance(value, dict):\n",
        "            flattened_json_dictionary.update(flatten_json(value, f\"{prefix}{key}_\"))\n",
        "        else:\n",
        "            # Update the flattened dictionary with the current key:value item\n",
        "            flattened_json_dictionary[f\"{prefix}{key}\"] = value\n",
        "\n",
        "    return flattened_json_dictionary\n",
        "\n",
        "# Get Variables within the same row\n",
        "def get_targetVariable_with_sourceVariable_from_pandas_df(df, source_column, source_value, target_column_to_fetch_value):\n",
        "    # This function extract the specific value from a column given the input of a different column and it's reference value\n",
        "    #  Example sites dataframe include multiple data in its metadata, but only the id is necessary in this case,\n",
        "    #  therefore to get the site's ID we look for the id using the site name of interest\n",
        "    #  E.g. get_targetVariable_with_sourceVariable_from_pandas_df(df, 'site_name', 'file_system', 'site_id')\n",
        "    try:\n",
        "        result = df.loc[df[source_column] == source_value, target_column_to_fetch_value].iloc[0]\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f'Value not founded, please check the input variables. Check the traceback below:{e}')\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_target_variable_with_source_variable_from_pyspark_df(spark, df, source_column, source_value, target_column_to_fetch_value):\n",
        "    # Same as above but for pyspark\n",
        "    try:\n",
        "        result = df.filter(col(source_column) == source_value).select(target_column_to_fetch_value).first()[0]\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f'Value not found, please check the input variables. Check the traceback below:{e}')\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbJ7LVGF1mRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# sharepoint_res_utils.py"
      ],
      "metadata": {
        "id": "nqfUOLod1xgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "#from utils.general_utils import flatten_json\n",
        "import requests\n",
        "\n",
        "\n",
        "# Get sites data in a flatten pandas dataframe (use for nested JSONS)\n",
        "def flatten_sites_metadata_to_pandas(json_data):\n",
        "    flattened_sites_data = [flatten_json(site_element, 'site_') for site_element in json_data['value']]\n",
        "    # Return data as pandas dataframe\n",
        "    return pd.DataFrame(flattened_sites_data)\n",
        "\n",
        "\n",
        "# Get sites data in a flatten pyspark dataframe (use for nested JSONS)\n",
        "def flatten_sites_metadata_to_spark(spark, json_data):\n",
        "    # Create the df using the JSON response\n",
        "    spark_df = spark.createDataFrame(json_data['value'])\n",
        "\n",
        "    # Flatten the DataFrame using explode when column is nested\n",
        "    for column in spark_df.columns:\n",
        "        if isinstance(spark_df.schema[column].dataType, (dict, list)):\n",
        "            spark_df = spark_df.withColumn(column, explode(col(column)))\n",
        "\n",
        "    # Rename columns with a prefix\n",
        "    for column in spark_df.columns:\n",
        "        spark_df = spark_df.withColumnRenamed(column, f\"site_{column}\")\n",
        "\n",
        "    return spark_df\n",
        "\n",
        "# Get sites files data in a flatten pandas dataframe (use for nested JSONS)\n",
        "def flatten_files_metadata_to_pandas(json_data, prefix):\n",
        "    flattened_sites_data = [flatten_json(site_element, prefix) for site_element in json_data['value']]\n",
        "    # Return data as pandas dataframe\n",
        "    return pd.DataFrame(flattened_sites_data)\n",
        "\n",
        "# Get sites files data in a flatten pyspark dataframe (use for nested JSONS)\n",
        "def flatten_files_metadata_to_pyspark(spark, json_data, prefix):\n",
        "    # Create the df using the JSON response\n",
        "    spark_df = spark.read.json(spark.sparkContext.parallelize([json_data]))\n",
        "\n",
        "    # Flatten the DataFrame using explode when column is nested\n",
        "    for column in spark_df.columns:\n",
        "        if isinstance(spark_df.schema[column].dataType, (dict, list)):\n",
        "            # If the column is of type dict or list, explode it\n",
        "            spark_df = spark_df.withColumn(column, col(column).alias(column + '_0')).selectExpr(\"*\", f\"inline_outer({column}) as {column}\")\n",
        "\n",
        "    # Rename columns with a prefix\n",
        "    for column in spark_df.columns:\n",
        "        spark_df = spark_df.withColumnRenamed(column, f\"{prefix}{column}\")\n",
        "\n",
        "    return spark_df\n",
        "\n",
        "\n",
        "# Upload files to specific site and folder in Sharepoint\n",
        "def upload_file_to_sharepoint(access_token, site_id, filename,folder_path = ''):\n",
        "\n",
        "    # Define upload request\n",
        "    upload_url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drive/root:{folder_path}/{filename}:/content\"\n",
        "\n",
        "    # Open file to write\n",
        "    with open(filename, 'rb') as file:\n",
        "        # Define header for request including bytes Content-Type\n",
        "        headers = {\n",
        "            'Authorization': access_token,\n",
        "            'Content-Type': 'application/octet-stream',\n",
        "        }\n",
        "        # PUT request to upload file to Sharepoint\n",
        "        response = requests.put(upload_url, headers=headers, data=file)\n",
        "\n",
        "        if response.status_code == 201 or response.status_code == 200:\n",
        "            print(f\"File '{filename}' uploaded successfully.\")\n",
        "        else:\n",
        "            print(f\"Failed to upload {filename}. please check Traceback below: {response.text}\")\n",
        "\n",
        "\n",
        "# Get sites metadata as response\n",
        "def get_sites_metadata_as_dataframe(spark,access_token, is_pyspark = True):\n",
        "    if is_pyspark == True:\n",
        "        return flatten_sites_metadata_to_spark(spark, get_graph_response('https://graph.microsoft.com/v1.0/sites', access_token).json())\n",
        "    else:\n",
        "        return flatten_sites_metadata_to_pandas(get_graph_response('https://graph.microsoft.com/v1.0/sites', access_token).json())\n",
        "\n",
        "\n",
        "        # Get specific Site ID for accessing that one\n",
        "        site_id = get_target_variable_with_source_variable_from_pyspark_df(df, 'site_name', 'file_system', 'site_id')\n",
        "        site_name = get_target_variable_with_source_variable_from_pyspark_df(df, 'site_name', 'file_system', 'site_name')\n",
        "        print(site_name)"
      ],
      "metadata": {
        "id": "vTI9rFkD1ud-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# yaml_utils.py"
      ],
      "metadata": {
        "id": "aH_Xs2kt10-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import os\n",
        "# Functions to support the library\n",
        "# Load config from YAML\n",
        "def load_config():\n",
        "    # Get the absolute path to the config.yml file\n",
        "    script_directory = os.path.dirname(os.path.abspath(__file__))\n",
        "    config_path = os.path.join(script_directory, 'config.yml')\n",
        "    with open(config_path, 'r') as config_file:\n",
        "        config = yaml.safe_load(config_file)\n",
        "    return config\n",
        "\n",
        "# Extract config variables from file\n",
        "def get_config_variables_from_file(config):\n",
        "    client_secret = config['share_point']['client_secret']\n",
        "    client_id = config['share_point']['client_id']\n",
        "    authority = config['share_point']['authority']\n",
        "    scope = config['share_point']['scope']\n",
        "    return client_secret, client_id, authority, scope\n"
      ],
      "metadata": {
        "id": "inPYyKEs1zKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main.py"
      ],
      "metadata": {
        "id": "TaU57a0r15jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import msal\n",
        "import yaml\n",
        "import json\n",
        "import requests\n",
        "import os\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
        "from pyspark.sql.functions import col, explode, coalesce, lit\n",
        "\n",
        "# Create class\n",
        "\n",
        "\n",
        "class Sharepoint_connector_API:\n",
        "\n",
        "    def __init__(self, connector=None, config_file_path=None):\n",
        "\n",
        "        # Get config variables from a specific connector.\n",
        "        # At the moment only databricks and YAML files are included as methods.\n",
        "        # Credentials created in azure portal (get them from Entra ID. Register a new app if needed(more details in readme.md))\n",
        "\n",
        "        # Create spark Session\n",
        "        self.spark = SparkSession.builder \\\n",
        "            .appName(\"Sharepoint_Connector_App\") \\\n",
        "            .getOrCreate()\n",
        "        # Config variables from Databricks\n",
        "        if connector.lower() == 'databricks':\n",
        "            self.client_id, self.client_secret, self.authority, self.scope = get_secret_variables()\n",
        "            print('fetched variables from script')\n",
        "        # Config variables from YAML\n",
        "        elif connector.lower() == 'yaml':\n",
        "            # Get config variables from file or else define as None\n",
        "            self.config = load_config(config_file_path)\n",
        "            self.client_id, self.client_secret, self.authority, self.scope = get_config_variables_from_file()\n",
        "        else:\n",
        "            ##############################################################################\n",
        "            ########################## START UNCOMMENT BEFORE GIT ########################\n",
        "            ##############################################################################\n",
        "\n",
        "            # self.client_id, self.client_secret, self.authority , self.scope = None\n",
        "\n",
        "            ##############################################################################\n",
        "            ########################## END UNCOMMENT BEFORE GIT ##########################\n",
        "            ##############################################################################\n",
        "\n",
        "            #####################################################################################################\n",
        "            ########################## START REMOVING LINES BELOW BEFORE UPLOAD TO GIT ##########################\n",
        "            #####################################################################################################\n",
        "\n",
        "            self.client_id = '28e6a33b-3653-4cad-be88-e77678e47f65'\n",
        "            self.client_secret = 'ws~8Q~sFWQpw3BYGSUVL.Twp4MeJk6-YZ1t0zb-W'\n",
        "            self.authority = 'https://login.microsoft.com/02c3ccc0-6680-4316-934b-97d503015046'\n",
        "            self.scope = ['https://graph.microsoft.com/.default']\n",
        "\n",
        "###################################################################################################\n",
        "########################## END REMOVING LINES BELOW BEFORE UPLOAD TO GIT ##########################\n",
        "###################################################################################################\n",
        "\n",
        "        self.access_token = self.get_authenticate_token(\n",
        "            self.client_id, self.client_secret, self.authority, self.scope)\n",
        "    # Functions to support the init function\n",
        "\n",
        "    # Create function to authenticate to sharepoint using authenticate token\n",
        "    def get_authenticate_token(self, client_id, client_secret, authority, scope):\n",
        "        # Getting access token\n",
        "        client = msal.ConfidentialClientApplication(\n",
        "            client_id, authority=authority, client_credential=client_secret)\n",
        "\n",
        "        # Try to lookup an access token in cache\n",
        "        token_cache = client.acquire_token_silent(scope, account=None)\n",
        "        # Assign token to access token for login\n",
        "        if token_cache:\n",
        "            access_token = 'Bearer ' + token_cache['access_token']\n",
        "            print('Access token fetched from Cache')\n",
        "        else:\n",
        "            token = client.acquire_token_for_client(scopes=scope)\n",
        "            access_token = 'Bearer ' + token['access_token']\n",
        "            print('Access token created using Azure AD')\n",
        "\n",
        "        return access_token\n",
        "\n",
        "    def get_graph_response(self, url, access_token):\n",
        "\n",
        "        # Define Header\n",
        "        headers = {\n",
        "            'Authorization': access_token\n",
        "        }\n",
        "\n",
        "        # Return get request using the access token\n",
        "        return requests.get(url=url, headers=headers)\n",
        "\n",
        "    # Get site metadata as pyspark df\n",
        "\n",
        "    def get_sites_metadata(self):\n",
        "\n",
        "        # Get sites metadata as response\n",
        "        url = f'https://graph.microsoft.com/v1.0/sites'\n",
        "        sites_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "\n",
        "        df = self.flatten_sublevel_metadata_to_spark(\n",
        "            sites_metadata_response.json(), 'value')\n",
        "\n",
        "        return df\n",
        "# Get flattend data in a flattened PySpark DataFrame (use for nested JSONS)\n",
        "    def flatten_sublevel_metadata_to_spark(self, json_data, key_to_convert):\n",
        "        # Create the schema by inferring data types\n",
        "        schema = StructType([StructField(key, StringType(), True) for key in json_data[key_to_convert][0]])\n",
        "\n",
        "        # Create DataFrame with schema\n",
        "\n",
        "        spark_df = self.spark.createDataFrame([], schema=schema)\n",
        "\n",
        "        # Populate the df with data\n",
        "        for row in json_data[key_to_convert]:\n",
        "            values = []\n",
        "            for key in schema.fieldNames():\n",
        "                # get values to populate df\n",
        "                value = row.get(key) if isinstance(row.get(key), dict) else row.get(key)\n",
        "                values.append(value)\n",
        "\n",
        "            spark_df = spark_df.union(self.spark.createDataFrame([tuple(values)], schema=schema))\n",
        "\n",
        "        return spark_df\n",
        "\n",
        "# Get specific site metadata df with site Name\n",
        "\n",
        "    def flatten_json_response_metadata_to_spark(self, json_response):\n",
        "        try:\n",
        "            # Read JSON string as a df\n",
        "            df = self.spark.read.json(\n",
        "                self.spark.sparkContext.parallelize([json_response]))\n",
        "\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_metadata_dataframe_with_site_name(self, df, site_name):\n",
        "        # Get sites metadata as response\n",
        "        url = f'https://graph.microsoft.com/v1.0/sites'\n",
        "        sites_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        df = self.get_sites_metadata()\n",
        "        site_id = get_target_variable_with_source_variable_from_pyspark_df(\n",
        "            self.spark, df, 'name', site_name, 'id')\n",
        "        # Use site_id for fetching that site_response\n",
        "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}\"\n",
        "        sites_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        df = self.flatten_json_response_metadata_to_spark(\n",
        "            sites_metadata_response.json())\n",
        "        return df\n",
        "\n",
        "# Get files metadata df\n",
        "    def get_all_files_metadata_with_site_name(self, df, site_name):\n",
        "        # Get sites metadata as response\n",
        "        url = f'https://graph.microsoft.com/v1.0/sites'\n",
        "        sites_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        df = self.get_sites_metadata()\n",
        "        site_id = get_target_variable_with_source_variable_from_pyspark_df(\n",
        "            self.spark, df, 'name', site_name, 'id')\n",
        "\n",
        "        # Use site_id for fetching that site_response\n",
        "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}\"\n",
        "        site_name_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        # Get df\n",
        "        df = self.flatten_json_response_metadata_to_spark(\n",
        "            site_name_metadata_response.json())\n",
        "\n",
        "        # Get new id\n",
        "        site_id = get_target_variable_with_source_variable_from_pyspark_df(\n",
        "            self.spark, df, 'name', site_name, 'id')\n",
        "\n",
        "        # Use new id to get new df\n",
        "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drive/root/children\"\n",
        "        site_files_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        df = self.flatten_sublevel_metadata_to_spark(\n",
        "            site_files_metadata_response.json(), 'value')\n",
        "\n",
        "        return df\n",
        "\n",
        "# Get specific file metadata with site_name\n",
        "    def get_specific_file_metadata_with_site_name(self, df, site_name, file_name):\n",
        "        # Get sites metadata as response\n",
        "        url = f'https://graph.microsoft.com/v1.0/sites'\n",
        "        sites_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        df = self.get_sites_metadata()\n",
        "        site_id = get_target_variable_with_source_variable_from_pyspark_df(\n",
        "            self.spark, df, 'name', site_name, 'id')\n",
        "\n",
        "        # Use site_id for fetching that site_response\n",
        "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}\"\n",
        "        site_name_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        # Get df\n",
        "        df = self.flatten_json_response_metadata_to_spark(\n",
        "            site_name_metadata_response.json())\n",
        "\n",
        "        # Get new site id\n",
        "        site_id = get_target_variable_with_source_variable_from_pyspark_df(\n",
        "            self.spark, df, 'name', site_name, 'id')\n",
        "\n",
        "        # Use new id to get new df\n",
        "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drive/root/children\"\n",
        "        site_files_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        df = self.flatten_sublevel_metadata_to_spark(\n",
        "            site_files_metadata_response.json(), 'value')\n",
        "        # Get file id\n",
        "        # file_ids = get_target_variable_with_source_variable_from_pyspark_df(self.spark, df, 'name', file_name, 'id')\n",
        "        file_ids_dict = {row['name']: row['id'] for row in df.collect()}\n",
        "        file_id = file_ids_dict.get('folder')\n",
        "        # Get file metadata as response\n",
        "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drive/items/{file_id}\"\n",
        "        file_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "\n",
        "        # Convert to df\n",
        "        df = self.flatten_json_response_metadata_to_spark(\n",
        "            file_metadata_response.json())\n",
        "\n",
        "        return df\n",
        "# Getget_all_specific_file_metadata_with_site_name\n",
        "\n",
        "    def get_all_specific_file_metadata_with_site_name(self, df, site_name):\n",
        "        # Initialize an empty list to store response dictionaries\n",
        "        response_list = []\n",
        "\n",
        "        # Get sites metadata as response\n",
        "        url = f'https://graph.microsoft.com/v1.0/sites'\n",
        "        sites_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        df = self.get_sites_metadata()\n",
        "        site_id = get_target_variable_with_source_variable_from_pyspark_df(\n",
        "            self.spark, df, 'name', site_name, 'id')\n",
        "\n",
        "        # Use site_id for fetching that site_response\n",
        "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}\"\n",
        "        site_name_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        # Get df\n",
        "        df = self.flatten_json_response_metadata_to_spark(\n",
        "            site_name_metadata_response.json())\n",
        "\n",
        "        # Get new site id\n",
        "        site_id = get_target_variable_with_source_variable_from_pyspark_df(\n",
        "            self.spark, df, 'name', site_name, 'id')\n",
        "\n",
        "        # Use new id to get new df\n",
        "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drive/root/children\"\n",
        "        site_files_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        df = self.flatten_sublevel_metadata_to_spark(\n",
        "            site_files_metadata_response.json(), 'value')\n",
        "        # Get file id\n",
        "        file_ids_dict = {row['name']: row['id'] for row in df.collect()}\n",
        "        file_id = file_ids_dict.get('folder')\n",
        "        # Get file metadata as response\n",
        "        url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drive/items/{file_id}\"\n",
        "        file_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "\n",
        "        # Append the file metadata response to the list\n",
        "        response_list.append(file_metadata_response.json())\n",
        "        print(file_metadata_response.json())\n",
        "        # Convert the list of dictionaries to a PySpark DataFrame\n",
        "        response_df = self.spark.createDataFrame(response_list)\n",
        "        print(len(response_list))\n",
        "        return response_df\n",
        "\n",
        "\n",
        "    # run function to be created accordingly\n",
        "\n",
        "    def test_run(self):\n",
        "\n",
        "        # Get sites metadata as response\n",
        "        url = f'https://graph.microsoft.com/v1.0/sites'\n",
        "        sites_metadata_response = self.get_graph_response(\n",
        "            url, self.access_token)\n",
        "        # get sites metadata df\n",
        "        df = self.get_sites_metadata()\n",
        "        # get site metadata df with site name\n",
        "        metadata_dataframe_with_site_name = self.get_metadata_dataframe_with_site_name(\n",
        "            df, 'file_system')\n",
        "        # get site metadata df with site name\n",
        "        files_metadata_dataframe_with_site_name = self.get_all_files_metadata_with_site_name(\n",
        "            df, 'file_system')\n",
        "        # Get specific file metadata df\n",
        "        specific_file_metadata = self.get_specific_file_metadata_with_site_name(\n",
        "            df, 'file_system', 'file_system_files_metadata.csv')\n",
        "\n",
        "        # Get all specific file metadata df\n",
        "        all_specific_file_metadata = self.get_all_specific_file_metadata_with_site_name(df, 'file_system')\n",
        "        # Get list of all files in this site\n",
        "\n",
        "        return df, specific_file_metadata, metadata_dataframe_with_site_name, files_metadata_dataframe_with_site_name, specific_file_metadata, all_specific_file_metadata\n",
        "\n",
        "\n",
        "# Testing running the class above with desire functions\n",
        "sp_connector = Sharepoint_connector_API('dummy_connector', None)\n",
        "df, specific_file_metadata, metadata_dataframe_with_site_name, files_metadata_dataframe_with_site_name, specific_file_metadata, all_specific_file_metadata = sp_connector.test_run()\n",
        "files_metadata_dataframe_with_site_name.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2148Apx12a4",
        "outputId": "08517e4e-3dfb-4fce-ef42-31b7dfd0ea5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access token created using Azure AD\n",
            "{'@odata.context': \"https://graph.microsoft.com/v1.0/$metadata#sites('xbpmf.sharepoint.com%2Cd8b00d65-d0c3-4b04-9a11-bb738275aa47%2C57216035-9a89-4f6f-8517-51cfdc6be081')/drive/items/$entity\", 'createdDateTime': '2023-12-19T09:43:58Z', 'eTag': '\"{5ADBB01F-63A2-48F0-B7FC-6F524CD7A461},1\"', 'id': '01SCS67TQ7WDNVVITD6BELP7DPKJGNPJDB', 'lastModifiedDateTime': '2023-12-19T09:43:58Z', 'name': 'folder', 'webUrl': 'https://xbpmf.sharepoint.com/sites/file_system/Shared%20Documents/folder', 'cTag': '\"c:{5ADBB01F-63A2-48F0-B7FC-6F524CD7A461},0\"', 'size': 8803, 'createdBy': {'user': {'email': 'Maxi@xbpmf.onmicrosoft.com', 'id': '8a763747-9377-4d44-aa0b-adc5c9e55010', 'displayName': 'Maxi'}}, 'lastModifiedBy': {'user': {'email': 'Maxi@xbpmf.onmicrosoft.com', 'id': '8a763747-9377-4d44-aa0b-adc5c9e55010', 'displayName': 'Maxi'}}, 'parentReference': {'driveType': 'documentLibrary', 'driveId': 'b!ZQ2w2MPQBEuaEbtzgnWqRzVgIVeJmm9PhRdRz9xr4IF7-qCj4Lf5Q64yaUZgyiv_', 'id': '01SCS67TV6Y2GOVW7725BZO354PWSELRRZ', 'name': 'Shared Documents', 'path': '/drive/root:', 'siteId': 'd8b00d65-d0c3-4b04-9a11-bb738275aa47'}, 'fileSystemInfo': {'createdDateTime': '2023-12-19T09:43:58Z', 'lastModifiedDateTime': '2023-12-19T09:43:58Z'}, 'folder': {'childCount': 1}, 'shared': {'scope': 'users'}}\n",
            "1\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------+-------------+\n",
            "|     createdDateTime|                eTag|                  id|lastModifiedDateTime|                name|              webUrl|                cTag|size|           createdBy|      lastModifiedBy|     parentReference|      fileSystemInfo|        folder|       shared|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------+-------------+\n",
            "|2023-12-19T09:43:58Z|\"{5ADBB01F-63A2-4...|01SCS67TQ7WDNVVIT...|2023-12-19T09:43:58Z|              folder|https://xbpmf.sha...|\"c:{5ADBB01F-63A2...|8803|{user={id=8a76374...|{user={id=8a76374...|{path=/drive/root...|{createdDateTime=...|{childCount=1}|{scope=users}|\n",
            "|2023-12-19T09:44:17Z|\"{93AD2050-C7F5-4...|01SCS67TSQECWZH5O...|2023-12-19T20:37:37Z|file_system_files...|https://xbpmf.sha...|\"c:{93AD2050-C7F5...|9131|{application={dis...|{application={dis...|{path=/drive/root...|{createdDateTime=...|          NULL|{scope=users}|\n",
            "|2023-12-18T17:00:09Z|\"{22780EC5-6B1B-4...|01SCS67TWFBZ4CEG3...|2023-12-19T20:37:31Z|file_system_metad...|https://xbpmf.sha...|\"c:{22780EC5-6B1B...| 611|{application={dis...|{application={dis...|{path=/drive/root...|{createdDateTime=...|          NULL|{scope=users}|\n",
            "|2023-12-18T16:57:40Z|\"{301E5ED8-31D0-4...|01SCS67TWYLYPDBUB...|2023-12-19T20:50:08Z|file_system_metad...|https://xbpmf.sha...|\"c:{301E5ED8-31D0...| 557|{application={dis...|{application={dis...|{path=/drive/root...|{createdDateTime=...|          NULL|{scope=users}|\n",
            "|2023-12-18T17:15:13Z|\"{B49E1720-2E0C-4...|01SCS67TRAC6PLIDB...|2023-12-19T20:37:34Z|sites_general_met...|https://xbpmf.sha...|\"c:{B49E1720-2E0C...|7206|{application={dis...|{application={dis...|{path=/drive/root...|{createdDateTime=...|          NULL|{scope=users}|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files_metadata_dataframe_with_site_name.show(5)\n"
      ],
      "metadata": {
        "id": "ynmbUzzu2ABE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80eb74a9-569c-41e3-b9ef-655afa963391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------+-------------+\n",
            "|     createdDateTime|                eTag|                  id|lastModifiedDateTime|                name|              webUrl|                cTag|size|           createdBy|      lastModifiedBy|     parentReference|      fileSystemInfo|        folder|       shared|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------+-------------+\n",
            "|2023-12-19T09:43:58Z|\"{5ADBB01F-63A2-4...|01SCS67TQ7WDNVVIT...|2023-12-19T09:43:58Z|              folder|https://xbpmf.sha...|\"c:{5ADBB01F-63A2...|8803|{user={id=8a76374...|{user={id=8a76374...|{path=/drive/root...|{createdDateTime=...|{childCount=1}|{scope=users}|\n",
            "|2023-12-19T09:44:17Z|\"{93AD2050-C7F5-4...|01SCS67TSQECWZH5O...|2023-12-19T20:37:37Z|file_system_files...|https://xbpmf.sha...|\"c:{93AD2050-C7F5...|9131|{application={dis...|{application={dis...|{path=/drive/root...|{createdDateTime=...|          NULL|{scope=users}|\n",
            "|2023-12-18T17:00:09Z|\"{22780EC5-6B1B-4...|01SCS67TWFBZ4CEG3...|2023-12-19T20:37:31Z|file_system_metad...|https://xbpmf.sha...|\"c:{22780EC5-6B1B...| 611|{application={dis...|{application={dis...|{path=/drive/root...|{createdDateTime=...|          NULL|{scope=users}|\n",
            "|2023-12-18T16:57:40Z|\"{301E5ED8-31D0-4...|01SCS67TWYLYPDBUB...|2023-12-19T20:50:08Z|file_system_metad...|https://xbpmf.sha...|\"c:{301E5ED8-31D0...| 557|{application={dis...|{application={dis...|{path=/drive/root...|{createdDateTime=...|          NULL|{scope=users}|\n",
            "|2023-12-18T17:15:13Z|\"{B49E1720-2E0C-4...|01SCS67TRAC6PLIDB...|2023-12-19T20:37:34Z|sites_general_met...|https://xbpmf.sha...|\"c:{B49E1720-2E0C...|7206|{application={dis...|{application={dis...|{path=/drive/root...|{createdDateTime=...|          NULL|{scope=users}|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------------+--------------------+--------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nbEiNQaoptsY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}