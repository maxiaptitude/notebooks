{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre requirements"
      ],
      "metadata": {
        "id": "W4DE2yluI6gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPyt3Vl5I8WU",
        "outputId": "ead104a2-648c-4845-a47b-ea89872fcc79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=7c0296f223dd8deadfd89548cbb98ac29b05dd33c81cc9ec6f36fe0247d70e70\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Config.yml\n",
        "database:\n",
        "  name: 'my_database'\n",
        "  connection:\n",
        "    host: 'localhost'\n",
        "    port: 5432\n",
        "    username: 'user'\n",
        "    password: 'password'\n",
        "mandatory_columns:\n",
        "  - age\n",
        "  - sex\n",
        "schema:\n",
        "  - {'field_name': 'id', 'field_type': 'StringType'}\n",
        "  - {'field_name': 'name', 'field_type': 'StringType'}\n",
        "  - {'field_name': 'age', 'field_type': 'IntegerType'}"
      ],
      "metadata": {
        "id": "rUh7QXTSddPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validators.py"
      ],
      "metadata": {
        "id": "O84Fui6LJSgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your_package/validators.py\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def validate_negative_values(df: DataFrame, column_name: str) -> None:\n",
        "    validation_result = df.filter(col(column_name) < 0)\n",
        "\n",
        "    if validation_result.count() > 0:\n",
        "        failed_rows = validation_result.select(column_name).collect()\n",
        "        error_message = {\n",
        "            'validator': 'NegativeValuesValidator',\n",
        "            'column': column_name,\n",
        "            'failed_values': [row[column_name] for row in failed_rows]\n",
        "        }\n",
        "        print(f'Validation Error: {error_message}')\n",
        "    else:\n",
        "        print(f'Validation Passed: No negative values in {column_name}.')\n"
      ],
      "metadata": {
        "id": "NVfVOklRJSPS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers.py"
      ],
      "metadata": {
        "id": "eFF79OEPJUon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def transform_square_column(df: DataFrame, column_name: str) -> DataFrame:\n",
        "    return df.withColumn(column_name, col(column_name) ** 2)"
      ],
      "metadata": {
        "id": "OFkJICKpJUax"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check date format"
      ],
      "metadata": {
        "id": "vtj_X8-EP1NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if date have correct format\n",
        "def check_date_format(df, column_to_check, date_format):\n",
        "\n",
        "  # Convert date column to string\n",
        "  formatted_date_column = date_format(df[column_to_check], date_format).cast(StringType())\n",
        "\n",
        "  # Conditional check if format of column is the same as the one specified in date_format\n",
        "  is_same_format_condition = when(formatted_date_column != df[column_to_check], df[column_to_check])\n",
        "\n",
        "  # Filter df with correct/incorrect formats\n",
        "  correct_format_df = df.filter(is_same_format_condition.isNull())\n",
        "  incorrect_format_df = df.filter(is_same_format_condition.isNotNull())\n",
        "\n",
        "    # Change return accordingly\n",
        "  return correct_format_df, incorrect_format_df"
      ],
      "metadata": {
        "id": "2zLrP8pbPyuf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_format_df, incorrect_format_df = check_date_format(df, 'date', 'yyyy-MM-dd')\n",
        "correct_format_df.show()\n",
        "incorrect_format_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "bA-lb3bVP6LX",
        "outputId": "802f6875-8f2d-404f-c39b-da84f1793080"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b37d791208a4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorrect_format_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincorrect_format_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_date_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yyyy-MM-dd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcorrect_format_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mincorrect_format_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check no duplicates"
      ],
      "metadata": {
        "id": "VkW_Eb_dP9SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if a column doesn't have duplicates\n",
        "def check_no_duplicates(df, column_to_check):\n",
        "\n",
        "    # Create a Window partition for all the unique values of the column to check\n",
        "    window_partition = Window().partitionBy(column_to_check)\n",
        "\n",
        "    # Add column with frequency per each unique value\n",
        "    df_with_frequency = df.withColumn('Frequency', count(column_to_check).over(window_partition))\n",
        "\n",
        "    # Filter rows where the count is greater than 1 (indicating duplicates)\n",
        "    df_wth_duplicates = df_with_frequency.filter(col('Frequency') > 1)\n",
        "    df_no_duplicates = df_with_frequency.filter(col('Frequency') == 1)\n",
        "    # Change return accordingly\n",
        "    return df_wth_duplicates, df_no_duplicates\n"
      ],
      "metadata": {
        "id": "dzCFhV2aQAWn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_wth_duplicates, df_no_duplicates = check_no_duplicates(df, 'province')\n",
        "df_wth_duplicates.show()\n",
        "df_no_duplicates.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "7CC0q0Q1QD5j",
        "outputId": "2a25d39c-9de6-4f32-8aa8-0b2c0c95771d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-5113e623dff0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_wth_duplicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_no_duplicates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_no_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'province'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_wth_duplicates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_no_duplicates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if null values"
      ],
      "metadata": {
        "id": "PtO_tZ2qQNj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_null_values(df, column_to_check):\n",
        "    # Filter df with/without nulls\n",
        "  df_with_null = df.filter(df[column_to_check].isNull())\n",
        "  df_without_null = df.filter(df[column_to_check].isNotNull())\n",
        "  return df_with_null, df_without_null\n"
      ],
      "metadata": {
        "id": "63iKn9_nQPe5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_wth_duplicates = df_wth_duplicates.withColumn('continent_name', f.when(f.col('continent_name') == 'North America', None).otherwise(f.col('continent_name')))\n",
        "df_wth_duplicates.show(1)\n",
        "df_with_null, df_without_null = check_null_values(df_wth_duplicates, 'continent_name')\n",
        "df_with_null.show(1)\n",
        "df_without_null.show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "qrIloYTtQUlM",
        "outputId": "a0468ef4-2fa1-41e7-9bf2-bea7ae3bb1b1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3256c7a495f8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_wth_duplicates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_wth_duplicates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'continent_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'continent_name'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'North America'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motherwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'continent_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_wth_duplicates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_with_null\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_without_null\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_null_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_wth_duplicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'continent_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_with_null\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_without_null\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_wth_duplicates' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check column type"
      ],
      "metadata": {
        "id": "eJRuA28UPB8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create spark session\n",
        "def check_column_types(df, schema_json):\n",
        "  # Create empty list for mismatched columns\n",
        "  mismatched_columns = []\n",
        "\n",
        "  # Check each field's type\n",
        "  for field in schema_json:\n",
        "    field_name = field['field_name']\n",
        "    expected_type = field['field_type']\n",
        "\n",
        "    # Get the actual type of the column\n",
        "    actual_type = type(df.schema[field_name].dataType).__name__\n",
        "\n",
        "    # Check if actual type matches expected type\n",
        "    if actual_type != expected_type:\n",
        "      # String with summary of the column\n",
        "      query = f'column {field_name} expected {expected_type}, but instead is {actual_type}'\n",
        "      mismatched_columns.append(query)\n",
        "      # To delete\n",
        "      print(query)\n",
        "  if mismatched_columns:\n",
        "    print('mismatched columns saved in JSON (FIX THIS)')\n",
        "    # Save output in json file or log.\n",
        "  else:\n",
        "    print('All column types match schema.')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ml82zEb7PFBA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get schema_json from config file\n",
        "spark = SparkSession.builder.appName('DataValidation').getOrCreate()\n",
        "data_validator = DataValidator(spark)\n",
        "config = data_validator.load_config('/content/config.yml')\n",
        "schema_json = config.get('schema', [])\n",
        "schema_json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXJKWgnRFJbw",
        "outputId": "da10d126-ce5c-40de-e135-a3379a03f453"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'field_name': 'id', 'field_type': 'StringType'},\n",
              " {'field_name': 'name', 'field_type': 'StringType'},\n",
              " {'field_name': 'age', 'field_type': 'IntegerType'}]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_column_types(source_df, schema_json)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "qJpYAWMfSP1Y",
        "outputId": "0c88a85c-f50c-4fd9-fe2d-bd488e6f38eb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-e2a741b071aa>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_column_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'source_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare Schemas"
      ],
      "metadata": {
        "id": "7vPkyu8GSoSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
        "\n",
        "def compare_schemas(spark, source_df, target_df):\n",
        "    # Get source/target schema\n",
        "    source_schema = source_df.schema\n",
        "    target_schema = target_df.schema\n",
        "\n",
        "    if source_schema == target_schema:\n",
        "        print('Schema for both dataframes match.')\n",
        "    else:\n",
        "        print('Schema do not match, please check details below')\n",
        "\n",
        "        # Get dictionary with field_name: field_type\n",
        "        source_columns = {field.name: field.dataType for field in source_schema.fields}\n",
        "        target_columns = {field.name: field.dataType for field in target_schema.fields}\n",
        "\n",
        "        # Get mismatched fields\n",
        "        mismatched_columns = set(source_columns.keys()) ^ set(target_columns.keys())\n",
        "\n",
        "        for column in mismatched_columns:\n",
        "            source_data_type = source_columns.get(column, 'not present in Source')\n",
        "            target_data_type = target_columns.get(column, 'not present in Target')\n",
        "            print(f'Column: {column}, Source DataType: {source_data_type}, Target DataType: {target_data_type}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y2kntdmKS3yz"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName('SchemaComparisonTest').getOrCreate()\n",
        "\n",
        "# Define schemas for source and target DataFrames\n",
        "source_schema = StructType([\n",
        "    StructField('id', IntegerType(), True),\n",
        "    StructField('name', StringType(), True),\n",
        "    StructField('age', IntegerType(), True)\n",
        "])\n",
        "\n",
        "target_schema = StructType([\n",
        "    StructField('id', IntegerType(), True),\n",
        "    StructField('name', StringType(), True),\n",
        "    StructField('score', FloatType(), True)\n",
        "])\n",
        "\n",
        "# Sample data for source and target DataFrames\n",
        "source_data = [(1, 'Alice', 25), (2, 'Bob', 30), (3, 'Charlie', 22)]\n",
        "target_data = [(1, 'Alice', 95.1), (2, 'Bob', 80.1), (3, 'Charlie', 75.1)]\n",
        "\n",
        "# Create source and target DataFrames\n",
        "source_df = spark.createDataFrame(source_data, schema=source_schema)\n",
        "target_df = spark.createDataFrame(target_data, schema=target_schema)\n",
        "\n",
        "# Print the content of source and target DataFrames\n",
        "print('Source DataFrame:')\n",
        "source_df.show()\n",
        "\n",
        "print('Target DataFrame:')\n",
        "target_df.show()\n",
        "\n",
        "# Compare Schemas\n",
        "compare_schemas(spark, source_df, target_df)\n",
        "# Stop the Spark session\n",
        "#spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kHjp1a7UCmb",
        "outputId": "820cd151-4225-4c38-f666-cdf631909e7b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source DataFrame:\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 25|\n",
            "|  2|    Bob| 30|\n",
            "|  3|Charlie| 22|\n",
            "+---+-------+---+\n",
            "\n",
            "Target DataFrame:\n",
            "+---+-------+-----+\n",
            "| id|   name|score|\n",
            "+---+-------+-----+\n",
            "|  1|  Alice| 95.1|\n",
            "|  2|    Bob| 80.1|\n",
            "|  3|Charlie| 75.1|\n",
            "+---+-------+-----+\n",
            "\n",
            "Schema do not match, please check details below\n",
            "Column: age, Source DataType: IntegerType(), Target DataType: not present in Target\n",
            "Column: score, Source DataType: not present in Source, Target DataType: FloatType()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check all required columns are in dataframe\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aEyTC_cfcblK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get columns from config file\n",
        "spark = SparkSession.builder.appName('DataValidation').getOrCreate()\n",
        "data_validator = DataValidator(spark)\n",
        "config = data_validator.load_config('/content/config.yml')\n",
        "mandatory_columns = config.get('mandatory_columns', [])\n",
        "\n",
        "def check_mandatory_columns(df, mandatory_columns):\n",
        "  if mandatory_columns == None:\n",
        "    print('n')\n",
        "  else:\n",
        "    # Get all field_name from df as list\n",
        "    fields = [field.name for field in df.schema.fields]\n",
        "\n",
        "    # Check all mandatory columns are present in df\n",
        "    missing_columns = set(mandatory_columns) - set(fields)\n",
        "\n",
        "  if missing_columns:\n",
        "      print(f'Missing columns: {', '.join(missing_columns)}')\n",
        "  else:\n",
        "      print('All mandatory columns are present in the dataframe.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "VCoBR8gZaRvi",
        "outputId": "88e52037-9cda-4bd0-a7d1-85d7c9cfa6c8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-8d8c133c8ee2>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    print(f'Missing columns: {', '.join(missing_columns)}')\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: expecting '}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_mandatory_columns(target_df, mandatory_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "ZE3cCyu7dYPf",
        "outputId": "c7ebb053-a933-4994-d66e-3a8a1f2a1d0d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c4a5e6e5f649>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_mandatory_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmandatory_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'check_mandatory_columns' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check column names are the same in Source & Target"
      ],
      "metadata": {
        "id": "_lvLqDIkAxAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get columns from config file\n",
        "spark = SparkSession.builder.appName('DataValidation').getOrCreate()\n",
        "data_validator = DataValidator(spark)\n",
        "config = data_validator.load_config('/content/config.yml')\n",
        "mandatory_columns = config.get('mandatory_columns', [])\n",
        "\n",
        "def check_sourceFields_equal_targetFields(source_df, target_df):\n",
        "    # Get Source/Target fields\n",
        "    source_field_names = [field.name for field in source_df.schema.fields]\n",
        "    target_field_names = [field.name for field in target_df.schema.fields]\n",
        "\n",
        "    # Check if different fields\n",
        "    wrong_fields = set(source_field_names) - set(target_field_names)\n",
        "\n",
        "    if wrong_fields:\n",
        "        print(f'''Columns names are not the same.\n",
        "Fields from Source not present in target:\n",
        "{wrong_fields}\n",
        "              ''')\n",
        "    else:\n",
        "        print('Columns names are the same in source and target dataframes.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "T6Iwbg1tAwvf",
        "outputId": "68290474-14cf-41c5-f347-9142b313deb7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6551c86f0fd1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get columns from config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataValidation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_validator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataValidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_validator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/config.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmandatory_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mandatory_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataValidator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "check_sourceFields_equal_targetFields(source_df, target_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "Xuxnx27GB-1u",
        "outputId": "10038cb3-045a-41dd-d643-5502e543db8d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-3fbe1311b0ad>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_sourceFields_equal_targetFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'check_sourceFields_equal_targetFields' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check condition by sql query\n",
        "\n",
        "does the constrains need to be in a table? can they be pre-defined in a config file?"
      ],
      "metadata": {
        "id": "iRr2HVdtAV7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute_sql_query_in_df(df,temp_table_name, query):\n",
        "  # Create temp delta table\n",
        "  df.createOrReplaceTempView(temp_table_name)\n",
        "\n",
        "  # Execute sql query in delta table\n",
        "  output = spark.sql(query)\n",
        "\n",
        "  # Return whatever the output is\n",
        "  return output\n"
      ],
      "metadata": {
        "id": "z7kswZOYAVwd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execute_sql_query_in_df(target_df,'temp_table_name', 'select count(*) from temp_table_name').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "zBaiyETXIqrR",
        "outputId": "385bc3e0-de18-4d47-e420-1ff1a0f75696"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c8c5e2d4941b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecute_sql_query_in_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'temp_table_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'select count(*) from temp_table_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'target_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare output of 2 sql queries in dfs"
      ],
      "metadata": {
        "id": "kfOhK-VXf64S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_source_and_target_output_by_sql(source_df, target_df, source_query, target_query):\n",
        "  # Run query for each dataframe\n",
        "  source_output = execute_sql_query_in_df(source_df, 'source_df', source_query)\n",
        "  print(source_output)\n",
        "  target_output = execute_sql_query_in_df(target_df,'target_df', target_query)\n",
        "  print(target_output)\n",
        "  # Compare if source & Target output are equal.\n",
        "  if source_output.exceptAll(target_output).count() == 0 and target_output.exceptAll(source_output).count() == 0:\n",
        "    print('both output are the same.')\n",
        "  else:\n",
        "    print('Source & Target outputs are different.')"
      ],
      "metadata": {
        "id": "YnFCfMdyISq7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_query = f'select * from source_df'\n",
        "target_query = f'select * from source_df'\n",
        "compare_source_and_target_output_by_sql(source_df, target_df, source_query, target_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "IKZOU1G8f6oa",
        "outputId": "d9a98706-66db-404a-d1d1-7ec8df8bf93f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4ca7d908ada6>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msource_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'select * from source_df'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtarget_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'select * from source_df'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcompare_source_and_target_output_by_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'source_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main.py"
      ],
      "metadata": {
        "id": "gX5jlDafI1UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, DataFrame\n",
        "import yaml\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n"
      ],
      "metadata": {
        "id": "gICVo0-WJIXq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#validators import validate_negative_values\n",
        "#from your_package.transformations import transform_square_column\n"
      ],
      "metadata": {
        "id": "6L8HzUAKJLVo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class DataValidator:\n",
        "    # Define init function\n",
        "    def __init__(self, spark: SparkSession):\n",
        "        self.spark = spark\n",
        "        self.data_df = None\n",
        "\n",
        "    # Get config file from local\n",
        "    def load_config(self, file_path):\n",
        "        with open(file_path, 'r') as config_file:\n",
        "          config = yaml.safe_load(config_file)\n",
        "        return config\n",
        "\n",
        "    # Apply transformations to df\n",
        "    def apply_transformations(self, transformations_config):\n",
        "        transformed_df = self.data_df\n",
        "\n",
        "        for transformation in transformations_config:\n",
        "            transformation_name = transformation['name']\n",
        "            transformation_params = transformation.get('params', {})\n",
        "\n",
        "            if transformation_name == 'square_column':\n",
        "                column_name = transformation_params.get('column')\n",
        "                transformed_df = transform_square_column(transformed_df, column_name)\n",
        "\n",
        "        self.data_df = transformed_df"
      ],
      "metadata": {
        "id": "AaPtKYPvJnEb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('DataValidation').getOrCreate()\n"
      ],
      "metadata": {
        "id": "J2F9SWpzJr6_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spark_session.stop()"
      ],
      "metadata": {
        "id": "OLo2n4LtKLjw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_validator = DataValidator(spark)\n"
      ],
      "metadata": {
        "id": "yQyN0Ak6J1tM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLTIX350LgBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = data_validator.load_config('/content/config.yml')\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hB8vsQfxEtC",
        "outputId": "34f525b0-095f-44b1-93a0-fd484c6a8f25"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'app_settings': {'title': 'My App',\n",
              "  'version': '1.0',\n",
              "  'options2': ['feature1', 'feature2'],\n",
              "  'options': {'feature1': True, 'feature2': False}},\n",
              " 'database': {'name': 'my_database',\n",
              "  'connection': {'host': 'localhost',\n",
              "   'port': 5432,\n",
              "   'username': 'user',\n",
              "   'password': 'password'}}}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dictionary from config\n",
        "config['app_settings'].get('options', {})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC4u-jujxJkh",
        "outputId": "fb324b3d-53c0-470c-d48b-77730f378348"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'feature1': True, 'feature2': False}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list from config\n",
        "config['app_settings'].get('options2', [])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5tDYtzsa23pb",
        "outputId": "edb15b50-b898-444d-b938-46fca0c9214f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'feature1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define schemas for source and target DataFrames\n",
        "source_schema = StructType([\n",
        "    StructField('id', IntegerType(), True),\n",
        "    StructField('name', StringType(), True),\n",
        "    StructField('age', IntegerType(), True)\n",
        "])\n",
        "\n",
        "source_data = [(1, 'Alice', 25), (2, 'Bob', 30), (3, 'Charlie', 22)]\n",
        "source_df = spark.createDataFrame(source_data, schema=source_schema)\n",
        "source_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmhDpAT4J4M5",
        "outputId": "94183aa3-9a7c-4d40-f470-6e0b6922a3fe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  1|  Alice| 25|\n",
            "|  2|    Bob| 30|\n",
            "|  3|Charlie| 22|\n",
            "+---+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_validator."
      ],
      "metadata": {
        "id": "Rab0fz4kKQ98"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}